# 《机器学习实战》学习笔记

[TOC]

## 写在前面

- 封面

  ![在这里插入图片描述](https://img-blog.csdnimg.cn/73b1da26341a49bda3aa06320e5ce774.png)

- 读后感

  1. 第一个分类的例子是象牙喙啄木鸟，说实话，不如西瓜。

- 传送门

  1. `np.tile(data,(x,y))`：此函数为扩展函数，data为要扩展的数据，类型为np类型数组，x扩展行数，y扩展列数

     ![在这里插入图片描述](https://img-blog.csdnimg.cn/60719cf57d844f7491feaf23844490d1.png)

  2. 

## 1. 机器学习基础

1. 数据挖掘十大算法（2007.12）：C4.5决策树、K-均值（K-mean）、支持向量机（SVM）、Apriori、最大期望算法（EM）、PageRank算法、AdaBoost算法、k-近邻算法（kNN）、朴素贝叶斯算法（NB）和分类回归树（CART）算法

2. 机器学习就是把无序的数据转换成有用的信息

3. 用于执行分类、回归、聚类和密度估计的机器学习算法

   ![在这里插入图片描述](https://img-blog.csdnimg.cn/0a0af05518a34f99acd20b92c0642f62.png)

4. Python语言唯一的不足是性能问题。Python程序运行的效率不如Java或者C代码高，但是我们可以使用Python调用C编译的代码。

## 2. k-近邻算法

1. k-近邻算法采用测量不同特征值之间的距离方法进行分类

   - 优点：精度高、对异常值不敏感、无数据输入假定。
   - 缺点：计算复杂度高、空间复杂度高。
   - 适用数据范围：数值型和标称型。

2. k-近邻算法：kNN

   - 它的工作原理是：存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。
   - 输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。
   - 一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。
   - 最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。

3. 例子

   - 输入数据：

     - 统计过很多电影的打斗镜头和接吻镜头

     - 6部电影的打斗和接吻镜头数

       ![在这里插入图片描述](https://img-blog.csdnimg.cn/581309fa889d48caab24e1ed88abf2dc.png)

     - 每部电影的打斗镜头数、接吻镜头数以及电影评估类型

       ![在这里插入图片描述](https://img-blog.csdnimg.cn/5cf717fcb13a4e2fba215e82dc288272.png)

     - 首先计算未知电影与样本集中其他电影的距离

       ![在这里插入图片描述](https://img-blog.csdnimg.cn/311ae2fa1ef141c897d590c593819580.png)

     - 假定k=3，则三个最靠近的电影依次是He’s Not Really into Dudes、Beautiful Woman和California Man

     - k-近邻算法按照距离最近的三部电影的类型，决定未知电影的类型，而这三部电影全是爱情片，因此我们判定未知电影是爱情片

4. k-近邻算法的一般流程

   ```
   (1) 收集数据：可以使用任何方法。
   (2) 准备数据：距离计算所需要的数值，最好是结构化的数据格式。
   (3) 分析数据：可以使用任何方法。
   (4) 训练算法：此步骤不适用于k-近邻算法。
   (5) 测试算法：计算错误率。
   (6) 使用算法：首先需要输入样本数据和结构化的输出结果，然后运行k-近邻算法判定输
   入数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理。
   ```

5. k-近邻算法的伪代码

   > 对未知类别属性的数据集中的每个点依次执行以下操作：
   > (1) 计算已知类别数据集中的点与当前点之间的距离；
   > (2) 按照距离递增次序排序；
   > (3) 选取与当前点距离最小的k个点；
   > (4) 确定前k个点所在类别的出现频率；
   > (5) 返回前k个点出现频率最高的类别作为当前点的预测分类。

6. kNN实现代码

   ```python
   def classify0(inX, dataSet, labels, k):
       dataSetSize = dataSet.shape[0]
       
       # 距离计算
       diffMat = tile(inX, (dataSetSize, 1)) - dataSet
       sqDiffMat = diffMat ** 2
       sqDistances = sqDiffMat.sum(axis=1)
       distances = sqDistances**0.5
       sortedDistIndicies = distances.argsort()
       
       # 选择距离最小的k个点
       classCount = {}
       for i in range(k):
           voteIlabel = labels[sortedDistIndicies[i]]
           classCount[voteIlabel] = classCount.get(voteIlabel, 0) + 1
       
       # 排序
       sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)
       return sortedClassCount[0][0]
   ```

7. 归一化特征值

   ```python
   def autoNorm(dataSet):
       minVals = dataSet.min(0)
       maxVals = dataSet.max(0)
       ranges = maxVals - minVals
       normDataSet = zeros(shape(dataSet))
       m = dataSet.shape[0]
       normDataSet = dataSet - tile(minVals, (m, 1))
       normDataSet = normDataSet / tile(ranges, (m, 1))
       return normDataSet, ranges, minVals
   ```

8. kNN的缺点：

   1. 实际使用这个算法时，算法的执行效率并不高。在手写数字分类案例中：因为算法需要为每个测试向量做2000次距离计算，每个距离计算包括了1024个维度浮点运算，总计要执行900次，此外，我们还需要为测试向量准备2MB的存储空间。是否存在一种算法减少存储空间和计算时间的开销呢？
   2. **k决策树** 就是k-近邻算法的优化版，可以节省大量的计算开销。
   3. k-近邻算法必须保存全部数据集，如果训练数据集的很大，必须使用大量的存储空间。此外，由于必须对数据集中的每个数据计算距离值，实际使用时可能非常耗时。
   4. k-近邻算法的另一个缺陷是它无法给出任何数据的基础结构信息，因此我们也无法知晓平均实例样本和典型实例样本具有什么特征。


## 3. 决策树

1. 决策树

   1. 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
   2. 缺点：可能会产生过度匹配问题。
   3. 适用数据类型：数值型和标称型。

2. 决策树的一般流程

   ```
   (1) 收集数据：可以使用任何方法。
   (2) 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。
   (3) 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
   (4) 训练算法：构造树的数据结构。
   (5) 测试算法：使用经验树计算错误率。
   (6) 使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据
   的内在含义。
   ```

3. 信息增益

   1. 在划分数据集之前之后信息发生的变化称为信息增益，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择
   2. 集合信息的度量方式称为香农熵或者简称为熵，这个名字来源于信息论之父克劳德·香农

4. 计算给定数据集的香农熵

   ```python
   from math import log
   
   def calcShannonEnt(dataSet):
       numEntries = len(dataSet)
       # 为所有可能的分类 创建字典
       labelCounts = {}
       for featVec in dataSet:
           currentLabel = featVec[-1]
           if currentLabel not in LabelCounts.keys():
               labelCounts[currentLabel] = 0
           labelCounts[currentLabel] += 1
       
       shannonEnt = 0.0
       for key in labelCounts:
           prob = float(labelCounts[key])/numEntries
           # 以2为底求对数
           shannonEnt -= prob*log(prob,2)
       return shannonEnt
   ```

5. 按照给定特征划分数据集

   ```python
   def splitDataSet(dataSet, axis, value):
       """输入：待划分的数据集、划分数据集的特征、需要返回减去对应特征的子集"""
       # 满足分类特征对应value的子集
       retDataSet = []
       for featVec in dataSet:
           # 每个样本的对应属性如果有指定的值，则将该属性剔除，保留其他属性，生成该特征对应值的【子集】
           if featVec[axis] == value:
               reducedFeatVec = featVec[:axis]
               reducedFeatVec.extend(featVec[axis+1:])
               retDataSet.append(reducedFeatVec)
           
       return retDataSet
   ```

6. 选择最好的数据集划分方式

   ```python
   def choosebestfeaturetosplit(dataset):   
       """就算出信息增益之后选取信息增益值最高的特征作为下一次分类的标准"""
       numfeatures=len(dataset[0])-1     #计算特征数量，列表【0】表示列的数量，-1是减去最后的类别特征label
       baseentropy=calcShannonEnt(dataset)   #计算数据集的信息熵
       bestinfogain=0.0;bestfeature=-1
       
       # 遍历特征的index
       for i in range(numfeatures):  
           # 求该特征一共有多少个取值
           featlist=[example[i] for example in dataset]
           uniquevals=set(featlist)   #确定某一特征下所有可能的取值
           newentropy=0.0
           
           # 遍历每一个对应特征的取值
           for value in uniquevals:
               subdataset=splitDataSet(dataset,i,value)#抽取在该特征的每个取值下其他特征的值组成新的子数据集
               prob=len(subdataset)/float(len(dataset))#计算该特征下的每一个取值对应的概率（或者说所占的比重）
               #计算该特征下每一个取值的子数据集的信息熵，即特征a中，a1占a1...an所有值的占比*信息熵，再求和
               newentropy +=prob*calcShannonEnt(subdataset)
               
           #计算每个特征的信息增益
           infogain=baseentropy-newentropy
           #  print("第%d个特征是的取值是%s，对应的信息增益值是%f"%((i+1),uniquevals,infogain))
           if(infogain>bestinfogain):
               bestinfogain=infogain
               bestfeature=i
       # print("第%d个特征的信息增益最大，所以选择它作为划分的依据，其特征的取值为%s,对应的信息增益值是%f"%((i+1),uniquevals,infogain))
       return bestfeature
   ```

7. 递归构建决策树

   1. 工作原理如下：得到原始数据集，然后基于最好的属性值划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分。第一次划分之后，数据将被向下传递到树分支的下一个节点，在这个节点上，我们可以再次划分数据。因此我们可以采用递归的原则处理数据集。
   2. 递归结束的条件是：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。如果所有实例具有相同的分类，则得到一个叶子节点或者终止块。任何到达叶子节点的数据必然属于叶子节点的分类
   3. 如果数据集已经处理了所有属性，但是类标签依然不是唯一的，此时我们需要决定如何定义该叶子节点，在这种情况下，我们通常会采用多数表决的方法决定该叶子节点的分类。
















学到 61 3.2


------


- :cloud: 我的CSDN：`https://blog.csdn.net/qq_21579045`
- :snowflake: 我的博客园：`https://www.cnblogs.com/lyjun/`
- :sunny: 我的Github：`https://github.com/TinyHandsome`
- :rainbow: 我的bilibili：`https://space.bilibili.com/8182822`
- :avocado: 我的思否：`https://segmentfault.com/u/liyj`
- :tomato: 我的知乎：`https://www.zhihu.com/people/lyjun_`
- :penguin: 粉丝交流群：1060163543，神秘暗号：为干饭而来

碌碌谋生，谋其所爱。:ocean:              @李英俊小朋友
