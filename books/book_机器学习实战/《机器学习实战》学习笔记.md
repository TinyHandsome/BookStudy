# 《机器学习实战》学习笔记

[TOC]

## 写在前面

- 封面

  ![在这里插入图片描述](https://img-blog.csdnimg.cn/73b1da26341a49bda3aa06320e5ce774.png)

- 读后感

  1. 第一个分类的例子是象牙喙啄木鸟，说实话，不如西瓜。

- 传送门

  1. `np.tile(data,(x,y))`：此函数为扩展函数，data为要扩展的数据，类型为np类型数组，x扩展行数，y扩展列数

     ![在这里插入图片描述](https://img-blog.csdnimg.cn/60719cf57d844f7491feaf23844490d1.png)

  2. 

## 1. 机器学习基础

1. 数据挖掘十大算法（2007.12）：C4.5决策树、K-均值（K-mean）、支持向量机（SVM）、Apriori、最大期望算法（EM）、PageRank算法、AdaBoost算法、k-近邻算法（kNN）、朴素贝叶斯算法（NB）和分类回归树（CART）算法

2. 机器学习就是把无序的数据转换成有用的信息

3. 用于执行分类、回归、聚类和密度估计的机器学习算法

   ![在这里插入图片描述](https://img-blog.csdnimg.cn/0a0af05518a34f99acd20b92c0642f62.png)

4. Python语言唯一的不足是性能问题。Python程序运行的效率不如Java或者C代码高，但是我们可以使用Python调用C编译的代码。

## 2. k-近邻算法

1. k-近邻算法采用测量不同特征值之间的距离方法进行分类

   - 优点：精度高、对异常值不敏感、无数据输入假定。
   - 缺点：计算复杂度高、空间复杂度高。
   - 适用数据范围：数值型和标称型。

2. k-近邻算法：kNN

   - 它的工作原理是：存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。
   - 输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。
   - 一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。
   - 最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。

3. 例子

   - 输入数据：

     - 统计过很多电影的打斗镜头和接吻镜头

     - 6部电影的打斗和接吻镜头数

       ![在这里插入图片描述](https://img-blog.csdnimg.cn/581309fa889d48caab24e1ed88abf2dc.png)

     - 每部电影的打斗镜头数、接吻镜头数以及电影评估类型

       ![在这里插入图片描述](https://img-blog.csdnimg.cn/5cf717fcb13a4e2fba215e82dc288272.png)

     - 首先计算未知电影与样本集中其他电影的距离

       ![在这里插入图片描述](https://img-blog.csdnimg.cn/311ae2fa1ef141c897d590c593819580.png)

     - 假定k=3，则三个最靠近的电影依次是He’s Not Really into Dudes、Beautiful Woman和California Man

     - k-近邻算法按照距离最近的三部电影的类型，决定未知电影的类型，而这三部电影全是爱情片，因此我们判定未知电影是爱情片

4. k-近邻算法的一般流程

   ```
   (1) 收集数据：可以使用任何方法。
   (2) 准备数据：距离计算所需要的数值，最好是结构化的数据格式。
   (3) 分析数据：可以使用任何方法。
   (4) 训练算法：此步骤不适用于k-近邻算法。
   (5) 测试算法：计算错误率。
   (6) 使用算法：首先需要输入样本数据和结构化的输出结果，然后运行k-近邻算法判定输
   入数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理。
   ```

5. k-近邻算法的伪代码

   > 对未知类别属性的数据集中的每个点依次执行以下操作：
   > (1) 计算已知类别数据集中的点与当前点之间的距离；
   > (2) 按照距离递增次序排序；
   > (3) 选取与当前点距离最小的k个点；
   > (4) 确定前k个点所在类别的出现频率；
   > (5) 返回前k个点出现频率最高的类别作为当前点的预测分类。

6. kNN实现代码

   ```python
   def classify0(inX, dataSet, labels, k):
       dataSetSize = dataSet.shape[0]
       
       # 距离计算
       diffMat = tile(inX, (dataSetSize, 1)) - dataSet
       sqDiffMat = diffMat ** 2
       sqDistances = sqDiffMat.sum(axis=1)
       distances = sqDistances**0.5
       sortedDistIndicies = distances.argsort()
       
       # 选择距离最小的k个点
       classCount = {}
       for i in range(k):
           voteIlabel = labels[sortedDistIndicies[i]]
           classCount[voteIlabel] = classCount.get(voteIlabel, 0) + 1
       
       # 排序
       sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)
       return sortedClassCount[0][0]
   ```

7. 归一化特征值

   ```python
   def autoNorm(dataSet):
       minVals = dataSet.min(0)
       maxVals = dataSet.max(0)
       ranges = maxVals - minVals
       normDataSet = zeros(shape(dataSet))
       m = dataSet.shape[0]
       normDataSet = dataSet - tile(minVals, (m, 1))
       normDataSet = normDataSet / tile(ranges, (m, 1))
       return normDataSet, ranges, minVals
   ```

8. kNN的缺点：

   1. 实际使用这个算法时，算法的执行效率并不高。在手写数字分类案例中：因为算法需要为每个测试向量做2000次距离计算，每个距离计算包括了1024个维度浮点运算，总计要执行900次，此外，我们还需要为测试向量准备2MB的存储空间。是否存在一种算法减少存储空间和计算时间的开销呢？
   2. **k决策树** 就是k-近邻算法的优化版，可以节省大量的计算开销。
   3. k-近邻算法必须保存全部数据集，如果训练数据集的很大，必须使用大量的存储空间。此外，由于必须对数据集中的每个数据计算距离值，实际使用时可能非常耗时。
   4. k-近邻算法的另一个缺陷是它无法给出任何数据的基础结构信息，因此我们也无法知晓平均实例样本和典型实例样本具有什么特征。


## 3. 决策树

1. 决策树

   1. 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
   2. 缺点：可能会产生过度匹配问题。
   3. 适用数据类型：数值型和标称型。

2. 决策树的一般流程

   ```
   (1) 收集数据：可以使用任何方法。
   (2) 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。
   (3) 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
   (4) 训练算法：构造树的数据结构。
   (5) 测试算法：使用经验树计算错误率。
   (6) 使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据
   的内在含义。
   ```

3. 信息增益















学到 54


------

- :cloud: 我的CSDN：https://blog.csdn.net/qq_21579045
- :snowflake: 我的博客园：https://www.cnblogs.com/lyjun/
- :sunny: 我的Github：https://github.com/TinyHandsome
- :rainbow: 我的bilibili：https://space.bilibili.com/8182822
- :penguin: 粉丝交流群：1060163543，神秘暗号：为干饭而来

碌碌谋生，谋其所爱。:ocean:              @李英俊小朋友

