# 《机器学习实战》学习笔记

[TOC]

## 写在前面

- 封面

  ![在这里插入图片描述](https://img-blog.csdnimg.cn/73b1da26341a49bda3aa06320e5ce774.png)

- 读后感

  1. 第一个分类的例子是象牙喙啄木鸟，说实话，不如西瓜。
  2. 

- 传送门

  1. `np.tile(data,(x,y))`：此函数为扩展函数，data为要扩展的数据，类型为np类型数组，x扩展行数，y扩展列数

     ![在这里插入图片描述](https://img-blog.csdnimg.cn/60719cf57d844f7491feaf23844490d1.png)

  2. 

## 1. 机器学习基础

1. 数据挖掘十大算法（2007.12）：C4.5决策树、K-均值（K-mean）、支持向量机（SVM）、Apriori、最大期望算法（EM）、PageRank算法、AdaBoost算法、k-近邻算法（kNN）、朴素贝叶斯算法（NB）和分类回归树（CART）算法

2. 机器学习就是把无序的数据转换成有用的信息

3. 用于执行分类、回归、聚类和密度估计的机器学习算法

   ![在这里插入图片描述](https://img-blog.csdnimg.cn/0a0af05518a34f99acd20b92c0642f62.png)

4. Python语言唯一的不足是性能问题。Python程序运行的效率不如Java或者C代码高，但是我们可以使用Python调用C编译的代码。

## 2. k-近邻算法

1. k-近邻算法采用测量不同特征值之间的距离方法进行分类

   - 优点：精度高、对异常值不敏感、无数据输入假定。
   - 缺点：计算复杂度高、空间复杂度高。
   - 适用数据范围：数值型和标称型。

2. k-近邻算法：kNN

   - 它的工作原理是：存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。
   - 输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。
   - 一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。
   - 最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。

3. 例子

   - 输入数据：

     - 统计过很多电影的打斗镜头和接吻镜头

     - 6部电影的打斗和接吻镜头数

       ![在这里插入图片描述](https://img-blog.csdnimg.cn/581309fa889d48caab24e1ed88abf2dc.png)

     - 每部电影的打斗镜头数、接吻镜头数以及电影评估类型

       ![在这里插入图片描述](https://img-blog.csdnimg.cn/5cf717fcb13a4e2fba215e82dc288272.png)

     - 首先计算未知电影与样本集中其他电影的距离

       ![在这里插入图片描述](https://img-blog.csdnimg.cn/311ae2fa1ef141c897d590c593819580.png)

     - 假定k=3，则三个最靠近的电影依次是He’s Not Really into Dudes、Beautiful Woman和California Man

     - k-近邻算法按照距离最近的三部电影的类型，决定未知电影的类型，而这三部电影全是爱情片，因此我们判定未知电影是爱情片

4. k-近邻算法的一般流程

   ```
   (1) 收集数据：可以使用任何方法。
   (2) 准备数据：距离计算所需要的数值，最好是结构化的数据格式。
   (3) 分析数据：可以使用任何方法。
   (4) 训练算法：此步骤不适用于k-近邻算法。
   (5) 测试算法：计算错误率。
   (6) 使用算法：首先需要输入样本数据和结构化的输出结果，然后运行k-近邻算法判定输
   入数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理。
   ```

5. k-近邻算法的伪代码

   > 对未知类别属性的数据集中的每个点依次执行以下操作：
   > (1) 计算已知类别数据集中的点与当前点之间的距离；
   > (2) 按照距离递增次序排序；
   > (3) 选取与当前点距离最小的k个点；
   > (4) 确定前k个点所在类别的出现频率；
   > (5) 返回前k个点出现频率最高的类别作为当前点的预测分类。

6. 代码

   ```python
   def classify0(inX, dataSet, labels, k):
       dataSetSize = dataSet.shape[0]
       
       # 距离计算
       diffMat = tile(inX, (dataSetSize, 1)) - dataSet
       sqDiffMat = diffMat ** 2
       sqDistances = sqDiffMat.sum(axis=1)
       distances = sqDistances**0.5
       sortedDistIndicies = distances.argsort()
       
       # 选择距离最小的k个点
       classCount = {}
       for i in range(k):
           voteIlabel = labels[sortedDistIndicies[i]]
           classCount[voteIlabel] = classCount.get(voteIlabel, 0) + 1
       
       # 排序
       sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)
       return sortedClassCount[0][0]
   ```

7. 

















学到 39


------

- :cloud: 我的CSDN：https://blog.csdn.net/qq_21579045
- :snowflake: 我的博客园：https://www.cnblogs.com/lyjun/
- :sunny: 我的Github：https://github.com/TinyHandsome
- :rainbow: 我的bilibili：https://space.bilibili.com/8182822
- :penguin: 粉丝交流群：1060163543，神秘暗号：为干饭而来

碌碌谋生，谋其所爱。:ocean:              @李英俊小朋友

