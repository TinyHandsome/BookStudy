{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([1.0, 2.0], name=\"a\")\n",
    "b = tf.constant([2.0, 3.0], name=\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(2,) dtype=float32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 5.], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(a.graph is tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.framework.ops.Graph object at 0x0000000016CFDC88>\n"
     ]
    }
   ],
   "source": [
    "print(a.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = tf.Graph()\n",
    "with g1.as_default():\n",
    "    v = tf.get_variable(\"v\", shape=[1], initializer=tf.zeros_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = tf.Graph()\n",
    "with g2.as_default():\n",
    "    v = tf.get_variable(\"v\", shape=[1], initializer=tf.ones_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g1) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    with tf.variable_scope(\"\", reuse=True):\n",
    "        print(sess.run(tf.get_variable(\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g2) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    with tf.variable_scope(\"\", reuse=True):\n",
    "        print(sess.run(tf.get_variable(\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(2+6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_1:0\", shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.constant是一个计算，这个计算的结果为一个张量，保存在变量a中\n",
    "a = tf.constant([1.0, 2.0], name=\"a\")\n",
    "b = tf.constant([2.0, 3.0], name=\"b\")\n",
    "result = tf.add(a, b, name=\"add\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 5.]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    print(result.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=(2,) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 5.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 5.]\n"
     ]
    }
   ],
   "source": [
    "print(result.eval(session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 5.]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "print(result.eval())\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "sess1 = tf.InteractiveSession(config=config)\n",
    "sess2 = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.random_normal([2, 3], stddev=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32_ref>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = tf.Variable(tf.zeros([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable_1:0' shape=(3,) dtype=float32_ref>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = tf.Variable(weights.initialized_value())\n",
    "w3 = tf.Variable(weights.initialized_value() * 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.957578]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 声明w1、w2两个变量。这里还通过seed参数设定了随机种子。\n",
    "# 这样可以保证每次运行得到的结果是一样的。\n",
    "w1 = tf.Variable(tf.random_normal((2, 3), stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal((3, 1), stddev=1, seed=1))\n",
    "\n",
    "# 暂时将输入的特征向量定义为一个常量。注意这里x是一个1x2的矩阵。\n",
    "x = tf.constant([[0.7, 0.9]])\n",
    "\n",
    "# 通过前向传播算法获得神经网络的输出。\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "sess = tf.Session()\n",
    "# 这里不能直接通过sess.run(y)来获取y的取值。\n",
    "# 因为w1和w2都还没有运行初始化过程。一下两行分别初始化了w1和w2两个变量。\n",
    "sess.run(w1.initializer)  # 初始化w1\n",
    "sess.run(w2.initializer)  # 初始化w2\n",
    "# 输出\n",
    "print(sess.run(y))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'w1_5:0' shape=(2, 3) dtype=float32_ref>\n",
      "<tf.Variable 'w1_5:0' shape=(2, 3) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev=1), name='w1')\n",
    "print(w1)\n",
    "w2 = tf.Variable(tf.random_normal([2, 3], dtype=tf.float32, stddev=1), name='w2')\n",
    "w1.assign(w2)\n",
    "print(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Assign_8:0' shape=(2, 2) dtype=float32_ref>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev=1), name='w1')\n",
    "w2 = tf.Variable(tf.random_normal([2, 2], stddev=1), name='w2')\n",
    "# 下面这句会报维度不匹配 的错误\n",
    "# tf.assign(w1, w2)\n",
    "# 这一句可以被成功执行\n",
    "tf.assign(w1, w2, validate_shape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.957578]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))\n",
    "\n",
    "#定义placeholder作为存放输入数据的地方。这里的维度也不应要定义。\n",
    "#但如果维度是确定的，那么给出维度可以降低出错的概率。\n",
    "x = tf.placeholder(tf.float32, shape=(1, 2), name=\"input\")\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "sess = tf.Session()\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "# 下面一行将报错\n",
    "# print(sess.run(y))\n",
    "\n",
    "# 这一行不会报错\n",
    "print(sess.run(y,feed_dict={x: [[0.7, 0.9]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.957578 ]\n",
      " [1.1537654]\n",
      " [3.1674924]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))\n",
    "\n",
    "#定义placeholder作为存放输入数据的地方。这里的维度也不应要定义。\n",
    "#但如果维度是确定的，那么给出维度可以降低出错的概率。\n",
    "x = tf.placeholder(tf.float32, shape=(3, 2), name=\"input\")\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "sess = tf.Session()\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "# 下面一行将报错\n",
    "# print(sess.run(y))\n",
    "\n",
    "# 这一行不会报错\n",
    "print(sess.run(y,feed_dict={x: [[0.7, 0.9], [0.1, 0.4], [0.5, 0.8]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t init_op\t json\t sess\t tf\t w1\t w2\t x\t y\t \n",
      "yapf_reformat\t \n"
     ]
    }
   ],
   "source": [
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]]\n",
      "[[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]]\n",
      "After 0 training step(s), cross entropy on all data is 1.89805\n",
      "After 1000 training step(s), cross entropy on all data is 0.655075\n",
      "After 2000 training step(s), cross entropy on all data is 0.626172\n",
      "After 3000 training step(s), cross entropy on all data is 0.615096\n",
      "After 4000 training step(s), cross entropy on all data is 0.610309\n",
      "After 5000 training step(s), cross entropy on all data is 0.608679\n",
      "After 6000 training step(s), cross entropy on all data is 0.608231\n",
      "After 7000 training step(s), cross entropy on all data is 0.608114\n",
      "[[ 0.08208176  0.5239096   1.7477902 ]\n",
      " [-2.2339835  -0.20824055  1.0780488 ]]\n",
      "[[-0.4916839]\n",
      " [ 0.4147874]\n",
      " [-1.003934 ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 通过numpy生成模拟数据集\n",
    "from numpy.random import RandomState\n",
    "\n",
    "# 定义训练数据batch的大小\n",
    "batch_size = 8\n",
    "\n",
    "# 定义神经网络的参数，这里还是沿用3.4.2小节中给出的神经网络结构。\n",
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))\n",
    "\n",
    "# 在shape的一个维度上使用None 可以方便使用不同的batch大小。在训练时，需要把数据分\n",
    "# 成比较小的batch，但是在测试时，可以一次性使用全部的数据。当数据集比较小时这样比\n",
    "# 较方便测试，但数据集比较大时，将大量数据放入一个batch可能会导致内存溢出。\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2), name='x-input')\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1), name='y-input')\n",
    "\n",
    "# 定义神经网络前向传播的过程\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "# 定义损失函数和反向传播的算法。\n",
    "y = tf.sigmoid(y)\n",
    "cross_entropy = -tf.reduce_mean(\n",
    "    y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)) \n",
    "    + (1-y_) * tf.log(tf.clip_by_value(1-y, 1e-10, 1.0))\n",
    ")\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "\n",
    "# 通过随机数生成一个模拟数据集。\n",
    "rdm = RandomState(1)\n",
    "dataset_size = 128\n",
    "X = rdm.rand(dataset_size, 2)\n",
    "# 定义规则来给出样本的标签。在这里所有x1 + x2 < 1 的样例都被认为是正样本\n",
    "# （比如零件合格），而其他为负样本（比如零件不合格）。和TensorFlow游乐场中的\n",
    "# 表示法不大一样的地方是，在这里使用0来表示负样本，1来表示正样本。大部分解决\n",
    "# 分类问题的神经网络都会采用0和1的表示方法。\n",
    "Y = [[int(x1+x2<1)] for (x1, x2) in X]\n",
    "\n",
    "# 创建一个会话来运行Tensorflow程序。\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    # 初始化变量。\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    print(sess.run(w1))\n",
    "    print(sess.run(w2))\n",
    "    \n",
    "    # 设置训练的轮数\n",
    "    STEPS = 8000\n",
    "    for i in range(STEPS):\n",
    "        # 每次选取batch_size个样本进行训练。\n",
    "        start = (i * batch_size) % dataset_size\n",
    "        end = min(start+batch_size, dataset_size)\n",
    "        \n",
    "        # 通过选取的样本训练神经网络并更新参数。\n",
    "        sess.run(train_step, feed_dict={x: X[start: end], y_: Y[start: end]})\n",
    "        if i % 1000 == 0:\n",
    "            # 每隔一段时间计算在所有数据上的交叉熵并输出。\n",
    "            total_cross_entropy = sess.run(\n",
    "                cross_entropy, feed_dict={x: X, y_: Y}\n",
    "            )\n",
    "            print(\"After %d training step(s), cross entropy on all data is %g\" % (i, total_cross_entropy))\n",
    "            \n",
    "    print(sess.run(w1))\n",
    "    print(sess.run(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.5 2.5 3. ]\n",
      " [4.  4.5 4.5]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(tf.clip_by_value(v, 2.5, 4.5).eval(session = sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2 = tf.constant([1.0, 2.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.        0.6931472 1.0986123]\n"
     ]
    }
   ],
   "source": [
    "print(tf.log(v2).eval(session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_mean(v).eval(session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True  True]\n",
      "[4. 3. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "v1 = tf.constant([1.0, 2.0, 3.0, 4.0])\n",
    "v2 = tf.constant([4.0, 3.0, 2.0, 1.0])\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "print(tf.greater(v1, v2).eval())\n",
    "\n",
    "print(tf.where(tf.greater(v1, v2), v1, v2).eval())\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.019347 ]\n",
      " [1.0428089]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from numpy.random import RandomState\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "# 两个输入节点\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2), name='x_input')\n",
    "# 回归问题一般只有一个输出节点\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1), name='y_input')\n",
    "\n",
    "# 定义了一个单层的神经网络前向传播的过程，这里就是简单加权和。\n",
    "w1 = tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1))\n",
    "y = tf.matmul(x, w1)\n",
    "\n",
    "# 定义预测多了和预测少了的成本\n",
    "loss_less = 10\n",
    "loss_more = 1\n",
    "loss = tf.reduce_sum(tf.where(tf.greater(y, y_), (y - y_) * loss_more, (y_ - y) * loss_less))\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "# 通过随机数生成一个模拟数据\n",
    "rdm = RandomState(1)\n",
    "dataset_size = 128\n",
    "X = rdm.rand(dataset_size, 2)\n",
    "\n",
    "# 设置回归的正确值为两个输入的和加上一个随机量。之所以要加上一个随机量是为了\n",
    "# 加入不可预测的噪音，否则不同损失函数的意义就不大了，因为不同损失函数都会在能\n",
    "# 完全预测正确的时候最低。一般来说噪音为一个均值为0的小量，所以这里的噪音设置为\n",
    "# -0.05 ~ 0.05 的随机数。\n",
    "\n",
    "Y = [[x1 + x2 + rdm.rand()/10.0 - 0.05] for (x1, x2) in X]\n",
    "\n",
    "# 训练神经网络\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 5000\n",
    "    for i in range(STEPS):\n",
    "        start = (i * batch_size) % dataset_size\n",
    "        end = min(start + batch_size, dataset_size)\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "    print(sess.run(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "7.5\n"
     ]
    }
   ],
   "source": [
    "weights = tf.constant([[1.0, -2.0], [-3.0, 4.0]])\n",
    "with tf.Session() as sess:\n",
    "    # 输出为l1正则化，其中0.5为正则化项的权重。\n",
    "    print(sess.run(tf.contrib.layers.l1_regularizer(0.5)(weights)))\n",
    "    # 输出为l2正则化\n",
    "    print(sess.run(tf.contrib.layers.l2_regularizer(0.5)(weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 获取一层神经网络边上的权重，并将这个权重的L2正则化损失加入名称为'losses'的集合中\n",
    "def get_weight(shape, lambda1):\n",
    "    # 生成一个变量。\n",
    "    var = tf.Variable(tf.random_normal(shape), dtype=tf.float32)\n",
    "    \n",
    "    # add_to_collection函数将这个新生变量的L2正则化损失项加入集合\n",
    "    # 这个函数的第一个参数'losses'是集合的名字，第二个参数是要加入这个集合的内容。\n",
    "    tf.add_to_collection(\n",
    "        'losses', tf.contrib.layers.l2_regularizer(lambda1)(var)\n",
    "    )\n",
    "    # 返回生成的变量\n",
    "    return var\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None. 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "batch_size = 8\n",
    "# 定义了每一层网络中节点的个数\n",
    "layer_dimension = [2, 10, 10, 10, 1]\n",
    "# 神经网络的层数\n",
    "n_layers = len(layer_dimension)\n",
    "\n",
    "# 这个变量维护前向传播时最深层的节点，开始的时候就是输入层。\n",
    "cur_layer = x\n",
    "# 当前层的节点个数\n",
    "in_dimension = layer_dimension[0]\n",
    "\n",
    "# 通过一个循环来生成5层全连接的神经网络结构\n",
    "for i in range(1, n_layers):\n",
    "    # layer_dimension[i]为下一层的节点个数。\n",
    "    out_dimension = layer_dimension[i]\n",
    "    # 生成当前层中权重的变量，并将这个变量的L2正则化损失加入计算图上的集合\n",
    "    weight = get_weight([in_dimension, out_dimension], 0.001)\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[out_dimension]))\n",
    "    # 使用ReLU激活函数\n",
    "    cur_layer = tf.nn.relu(tf.matmul(cur_layer, weight) + bias)\n",
    "    # 进入下一层之前将下一层的节点个数更新为当前层节点个数\n",
    "    in_dimension = layer_dimension[i]\n",
    "\n",
    "# 在定义神经网络前向传播的同时已经将所有的L2正则化损失加入了图上的集合\n",
    "# 这里只需要计算刻画模型在训练数据上表现的损失函数\n",
    "mse_loss = tf.reduce_mean(tf.square(y_ - cur_layer))\n",
    "\n",
    "# 将均方误差损失函数加入损失集合\n",
    "tf.add_to_collection('losses', mse_loss)\n",
    "\n",
    "# get_collection返回一个列表，这个列表是所有这个集合中的元素。在这个样例中，\n",
    "# 这些元素就是损失函数的不同部分，将它们加起来就可以得到最终的损失函数。\n",
    "loss = tf.add_n(tf.get_collection('losses'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0]\n",
      "[5.0, 4.5]\n",
      "[10.0, 4.555]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 定义一个变量用于计算滑动平均，这个变量的初始值为0.注意这里手动制定了变量的\n",
    "# 类型为tf.float32，因为所有需要计算滑动平均的变量必须是实数形。\n",
    "v1 = tf.Variable(0, dtype=tf.float32)\n",
    "# 这里step变量模拟神经网络中迭代的轮数，可以用于动态控制衰减率\n",
    "step = tf.Variable(0, trainable=False)\n",
    "\n",
    "# 定义一个滑动平均的类（class）。初始化时给定了衰减率（0.99）和控制衰减率的变量step\n",
    "ema = tf.train.ExponentialMovingAverage(0.99, step)\n",
    "\n",
    "# 定义一个更新变量滑动平均的操作。这里需要给定一个列表，每次执行这个操作时\n",
    "# 这个列表中的变量都会被更新。\n",
    "maintain_averages_op = ema.apply([v1])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 初始化所有的变量\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # 通过ema.average(v1)获取滑动平均之后变量的取值。在初始化之后变量v1的值\n",
    "    # 和v1的滑动平均都为0\n",
    "    print(sess.run([v1, ema.average(v1)]))\n",
    "    \n",
    "    # 更新变量v1的值到5\n",
    "    sess.run(tf.assign(v1, 5))\n",
    "    # 更新v1的滑动平均值。衰减率为min{0.99, (1+step)/(10+step) = 0.1 }=1\n",
    "    # 所以v1的滑动平均会被更新为0.1*0+0.9*5=4.5\n",
    "    sess.run(maintain_averages_op)\n",
    "    print(sess.run([v1, ema.average(v1)]))\n",
    "    \n",
    "    # 鞥更新step的值为100000.\n",
    "    sess.run(tf.assign(step, 100000))\n",
    "    # 更新v1的值为10\n",
    "    sess.run(tf.assign(v1, 10))\n",
    "    # 更新v1的滑动平均值。衰减率为min({0.99, (1+step)/(10+step=0.999}=0.99\n",
    "    # 所以v1的滑动平均会被更新为0.99*4.5+0.01*10=4.555\n",
    "    sess.run(maintain_averages_op)\n",
    "    print(sess.run([v1, ema.average(v1)]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
