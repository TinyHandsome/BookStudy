# 《动手学深度学习（pytorch版）》学习笔记

[TOC]

## 写在前面

- 封面 | 摘要 | 关键词
- 读后感
- 摘抄
  - pandas技巧
    - `pd.get_dummies()`：生成哑变量，是利用pandas实现one hot encode的方式，参数 `dummy_na=True` 能够把空值也当作一个元素，否则忽略控制
    - 

- 传送门

## 1. 引言

1. **参数**：参数可以被看作旋钮，旋钮的转动可以调整程序的行为。

2. **模型**：任一调整参数后的程序被称为模型（model）。

   通过操作参数而生成的所有不同程序（输入‐输出映射）的集合称为“模型族”。

3. **算法**：使用数据集来选择参数的元程序被称为学习算法（learning algorithm）。

4. **训练**：训练过程通常包含如下步骤：

   1. 从一个随机初始化参数的模型开始，这个模型基本没有“智能”；
   2. 获取一些数据样本（例如，音频片段以及对应的是或否标签）；
   3. 调整参数，使模型在这些样本中表现得更好；
   4. 重复第（2）步和第（3）步，直到模型在任务中的表现令人满意。

   ![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/f89373d5e2ef465db5c8fa1d1033815f.png)

5. 与传统机器学习方法相比，深度学习的一个主要优势是可以**处理不同长度的数据**。

6. 数据不仅要海量，还要正确。**Garbage in, garbage out.** 当数据不具有充分代表性，甚至包含了一些社会偏见时，模型就很有可能有偏见。

7. 用数据集通常可以分成两部分：训练数据集用于拟合模型参数，测试数据集用于评估拟合的模型。

8. 当一个模型在训练集上表现良好，但不能推广到 测试集时，这个模型被称为过拟合（overfitting）的。

9. 机器学习

   1. 监督学习过程

      ![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/9cfb1d5684ba45bda0ee19a25f98eefb.png)

      - 回归
      - 分类
      - 标注问题
      - 搜索
      - 推荐系统
      - 序列学习：标记和解析、自动语音识别、文本到语音、机器翻译

   2. 无监督学习

      - 聚类
      - 主成分分析
      - 因果关系和概率图模型
      - 生成对抗网络

   3. 与环境互动

      ![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/ffb268a7072e4d4f89cac8a8f1500e27.png)

   4. 强化学习

      ![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/bf243017429041f0a560ca4c94f9a8f1.png)

      - 当环境可被完全观察到时：强化学习问题被称为马尔可夫决策过程（markov decision process）。
      - 当状态不依赖于之前的操作时：我们称该问题为上下文赌博机（contextual bandit problem）。
      - 当没有状态，只有一组最初未知回报的可用动作时：这个问题就是经典的多臂赌博机（multi‐armed bandit problem）。

10. **机器学习**：研究计算机系统如何利用经验（通常是数据）来提高特定任务的性能。它结合了统计学、数据 挖掘和优化的思想。通常，它是被用作实现人工智能解决方案的一种手段。

11. **表示学习**：作为机器学习的一类，其研究的重点是如何自动找到合适的**数据表示方式**。深度学习是通过学习多层次的转换来进行的多层次的表示学习。

12. **深度学习**：深度学习的一个**关键优势**是它不仅取代了传统学习管道末端的浅层模型，而且还取代了劳动密集型的特征工程过程。此外，通过取代大部分特定领域的预处理，深度学习消除了以前分隔计算机视觉、语音识别、自然语言处理、医学信息学和其他应用领域的许多界限，为解决各种问题**提供了一套统一的工具**。

## 2. 预备知识

1. 张量：tensor，n维数组，numpy中的ndarray。

   张量表示一个由数值组成的数组，这个数组可能有多个维度。具有**一个轴**的张量对应数学上的**向量**（vector）； 具有**两个轴**的张量对应数学上的**矩阵**（matrix）；具有两个轴以上的张量没有特殊的数学名称。

   - 标量：`torch.tensor(3.0)`
   - 向量：`torch.tensor([0, 1, 2, 3])`

2. 广播机制（broadcasting mechanism）：

   1. 通过适当复制元素来扩展⼀个或两个数组，以便在转换之后，两个张量具有相同的形状；
   2. 对⽣成的数组执⾏按元素操作。

3. 节省内存：

   - `Y = Y + X`这样的操作会为Y新建内存地址
   - 而如果使用 `Y[:] = X + Y`则能够减少操作的内存开销

4. numpy和torch的张量转换：

   - `X.numpy()`
   - `torch.tensor(A)`

5. 张量算法的本质：

   - 任何按元素的一元运算都不会改变其操作数的形状，同样，给定具有相同形状的任意两个张量，任何按元素⼆元运算的结果都将是相同形状的张量。
   - 两个矩阵按元素乘法称为 `Hadamard积(Hadamard product, ⊙)`，即对应位置相乘
   - 张量乘以或加上⼀个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘

6. 















------


- :cloud: 我的CSDN：`https://blog.csdn.net/qq_21579045/`
- :snowflake: 我的博客园：`https://www.cnblogs.com/lyjun/`
- :sunny: 我的Github：`https://github.com/TinyHandsome/`
- :rainbow: 我的bilibili：`https://space.bilibili.com/8182822/`
- :avocado: 我的思否：`https://segmentfault.com/u/liyj/`
- :tomato: 我的知乎：`https://www.zhihu.com/people/lyjun_/`
- :potato: 我的豆瓣：`https://www.douban.com/people/lyjun_/`
- :penguin: 粉丝交流群：1060163543，神秘暗号：为干饭而来

碌碌谋生，谋其所爱。:ocean:              @李英俊小朋友
