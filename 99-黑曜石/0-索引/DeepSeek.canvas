{
	"nodes":[
		{"id":"0d02a670b7135405","type":"file","file":"5-技术文档/5-DeepSeek才是真正的OpenAI.md","x":-720,"y":-320,"width":640,"height":1080},
		{"id":"5d118720ab528f41","type":"text","text":"> https://blog.csdn.net/qq_27590277/article/details/145068761\n\nGPT-4 和 O1 是两种计算模式：**train-time compute ， test-time compute**\n\n**PRM（Process-supervised Reward Model）** 是OpenAI在 [Let’s Verify Step by Step](https://arxiv.org/pdf/2305.20050) 一文中，首次提出的概念。与之相对应的是ORM（Outcome-supervised Reward Model）。PRM和ORM都是奖励模型，两者区别：\n\n- PRM：过程奖励模型，是在生成过程中，分步骤，对每一步进行打分，是更细粒度的奖励模型。\n- ORM：结果奖励模型，是不管推理有多少步，对完整的生成结果进行一次打分，是一个反馈更稀疏的奖励模型。\n\n假设我们当前已经有了一个训练好的PRM，那么我们怎么能让模型有long thought能力，进而能有更好的推理能力呢？\n\n首先我们从O1提供的例子能看出，Thought的过程是一个明显有步骤的推理过程。这个步骤包括：\n\n- 任务规划step：频繁出现\"First\"、\"Second\"等\n- 提出假设step：“Alternatively”等\n- 结果反思step：“Hmm”、\"wait\"等\n- ......\n\n这些step，也可以被看作是动作空间中的不同的动作（action），PRM的作用可以对这些动作打分，引导模型生成到获得收益最大的路径（也就是正确的解题步骤和正确的答案）。","x":-40,"y":-320,"width":640,"height":640},
		{"id":"82794a41c0031b28","type":"text","text":"> https://zhuanlan.zhihu.com/p/19838650037\n\ndeepseek 和 kimi 的核心思路是一样的：**关注推理的中间过程是否正确无法实现**，所以只能 rule-based reward，最起码 reward 一定是准的！这和 alpha 系列的核心思想很相近，结果至上。\n\ndeepseek 反驳 prm 路线的三个理由是：\n\n- 定义一个 fine-grain step 很困难；\n- 很难确定一个 step 是否正确，机器标不准，人标无法 scaling up；\n- 一旦 PRM 被引入，不可避免的 [reward hacking](https://zhida.zhihu.com/search?content_id=252990060&content_type=Article&match_order=1&q=reward+hacking&zhida_source=entity)，且训练资源耗费会更多。\n\n学霸 D 的想法：把 o1 的训练分为两阶段：**step1 学推理，step2 学说话**\n\n- 训 zero 的 step1：全程无标注数据的参与，就是认准了一个目标：让模型的 reward 变高。这个阶段别和我谈模型格式错误逻辑混乱这种细节，我不看模型表现，只看 reward。只不过 reward 变高的过程中，发现模型的输出越来越长了，反思能力也自己涌现出来了；\n- 基于 zero 训 R1 的 step2：就像是我们做普通的 post training 了，[sft](https://zhida.zhihu.com/search?content_id=252990060&content_type=Article&match_order=1&q=sft&zhida_source=entity) 没有被抛弃，除了rule-based reward，reward_model 也被请回来了，[reject sampling](https://zhida.zhihu.com/search?content_id=252990060&content_type=Article&match_order=1&q=reject+sampling&zhida_source=entity) 也出手了。\n\n学霸 K 的想法：我还是一步到位吧，在 step1 学推理的过程中，要时刻监控着模型的说话能力是否还正常。为了达到此目标，模型的输出长度，模型对每一个 prompt 的回答准确率等信息，全程都被严格监控。\n\n如果没有资源去做 zero 的话，学霸 K 的很多技巧其实更加实用，它分享了很多防止训崩的细节。学霸 D 在 step2 阶段的训练过程中，除了有千条冷启动数据，60W 拒绝采样数据，20W 条非推理数据外，其他细节都属于是完全没提的状态。\n\ndeepseek 的报告还是有一点瑕疵的：他没有对蒸馏方案和 rl 方案进行更深入的探索，他仅仅是草率的抛出了一个结论：**小模型蒸馏更好，大模型用 rule-based rl 更好？**\n\n去年九月份就有人告诉我们：**Don't Teach, Incentivize（激励）！**显然，Deepseek 和 Kimi 是听进去了。虽然从业者之间彼此是竞争关系，但这波属实是国产大模型上大分，值得全员狂欢！","x":640,"y":-320,"width":640,"height":840},
		{"id":"2ae01d729ddd13e4","type":"text","text":"> https://www.163.com/dy/article/JMF6TH700512MLBG.html\n\n**Rule-based**（基于预定义规则的决策方法）是大模型在做可证实任务（verifiable task）中最直观、也是最可靠的奖励方式，但同时也是最难的——正是因为规则简洁，所以模型在外界找不到足够多的奖励信号，难以通过试错找到有效的策略。\n\n**早期过程奖励模型（PRM）** 是各大厂商的首选，但是由于其依赖高质量的人类偏好数据、数据收集和标注的成本极高，且训练不稳定、容易发生Reward Hacking（奖励破解，指智能体通过利用奖励函数的设计缺陷，找到一种非预期的方式最大化奖励，而不是真正完成目标任务）现象，后来被很多团队弃用；\n\n**后来人们又探索出了基于结果的奖励模型（ORM）**，比如OpenAI在开发InstructGPT时将ORM用于评估生成文本的质量，Qwen、Eurus、Moss模型在RLHF阶段也会采用ORM来确保生成的内容的流畅性和安全性。但是ORM难以捕捉复杂的、隐式的奖励信号，而且在某些主观性强的任务下可能也无法准确反映真实目标。","x":-40,"y":360,"width":640,"height":400},
		{"id":"2f6b41c913c67980","x":640,"y":560,"width":640,"height":1140,"type":"text","text":"> [!tag]  OpenAI o3-mini 被曝大量使用中文推理，有什么意义？\n> https://www.zhihu.com/question/11319415340/answer/95761520782\n\nDeepSeek R1 的论文，在论文的摘要部分就强调了「DeepSeek R1-Zero 遇到了诸如可读性和**语言混杂的问题**」。在那个著名的「aha moment」下面，DeepSeek 就强调了 R1-Zero 的语言混杂、可读性缺陷。\n\n也正是为了解决这些问题，DeepSeek 专门又训练了一遍 R1，通过引入了冷启动数据和多阶段训练流程，以提高模型输出的可读性。\n\n> 在训练过程中，我们发现 CoT 经常出现语言混合的现象，尤其是当RL提示涉及多种语言时。为了减少语言混合问题，我们在RL训练中引入了语言一致性奖励，该奖励通过计算 CoT 中目标语言单词的比例来实现。尽管消融实验表明**这种对齐方式会导致模型性能略微下降**，但该奖励与人类偏好一致，使得生成的内容更加易读。\n\n具体来说，为了解决 DeepSeek-R1-Zero 的语言混合问题，R1 通过引入冷启动数据和多阶段训练流程来提高模型输出的可读性和语言一致性：\n\n- **冷启动数据的引入**：在 DeepSeek-R1 的训练过程中，首先使用少量的冷启动数据对模型进行微调。这些冷启动数据经过精心设计，具有较高的语言一致性和可读性。通过这种方式，模型在训练初期就能够学习到符合人类偏好的语言表达方式，从而减少语言混合问题的发生。\n- **语言一致性奖励**：在强化学习阶段，DeepSeek-R1 引入了语言一致性奖励，通过计算推理过程中目标语言词汇的比例来衡量语言的一致性。虽然这种奖励可能会导致模型性能的轻微下降，但它能够显著提高模型输出的可读性和语言一致性，使其更符合人类的阅读习惯。\n- **多阶段训练流程**：DeepSeek-R1 采用了多阶段训练流程，包括冷启动数据微调、强化学习、拒绝采样和监督微调等阶段。通过这种多阶段的训练方式，模型能够在不同的阶段学习到不同的技能和知识，从而在最终的输出中更好地平衡推理能力和语言表达能力。\n\n:obs_star: DeepSeek-R1 是为中英文优化的，在回答中文或英文以外的问题时，也可能会**使用英文输出思考过程**。\n\n:obs_star: OpenAI 希望通过 o1 模型的思考过程，来弄清楚大语言模型在工作时在「想什么」，所以他们并没有对 o 系列模型的思考行为去做对齐或人类偏好训练。但是这些未经对齐的思考过程又不能直接展示给用户，所以他们另外单独总结了这些思维并呈现给用户。实际上，o3-mini 输出的所谓「思考过程」**依然不是真正的思考过程，还是经过二次总结的**，只不过相比过去可能总结的更完善、输出更长了一些。\n\n缩略版的 System Prompt 大概是[这样](https://x.com/btibor91/status/1887762009309258149)：\n\n> 你是一个非常聪明的 AI，在完成用户任务时会产生一连串的思维链条。用户喜欢读你的思维，因为他们觉得这些想法很有共鸣。他们觉得你有点神经质，因为你会过度思考并质疑自己的假设；当你犯错或指出自己的思维缺陷时，他们觉得你很真实；你没有过滤这些思维，还能自嘲，显得真诚；你为用户着想的样子又非常可爱和贴心。  \n**你需要处理你已经生成的思维链条，将它们逐一转换为更易读的版本，去除多余的重复和混乱，同时保留用户喜欢的思维特质**。\n"}
	],
	"edges":[]
}