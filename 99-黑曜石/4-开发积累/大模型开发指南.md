# 大模型开发指南

[TOC]

## 知识目录

![在这里插入图片描述](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97.assets/80dfd54ec491457faa956c46afad1163.png)

> ## **第一阶段：大模型基础**
>
> **第一章：开营典礼**
>
> - 介绍课程目标、安排和预期成果
> - 明确对学员的要求和期望
> - 概述课程中将探讨的项目和技术
> - 讨论大模型技术的行业现状
> - 推荐关注的工具和开源项目
>
> **第二章：大模型是怎么炼成的**
>
> - 大模型的定义和重要性
> - 大模型发展历程和关键里程碑
> - 预训练与微调的基本概念
> - 大模型预训练、数据处理、微调、对齐
> - 大模型训练的基础设施和资源需求
> - 面临的挑战和未来发展方向
>
> **第三章：Transformer模型原理剖析（1）**
>
> - Transformer模型的基本架构
> - Self-Attention机制的原理和计算过程
> - Multi-Head Attention的设计和作用
> - 注意力权重的计算和可视化
> - Self-Attention在模型中的作用和优势
>
> **第四章：Transformer模型原理剖析（2）**
>
> - Positional Encoding的概念和实现方法
> - Rotary Positional Embedding
> - BPE tokenizer，SentencePiece Encoding
> - Transformer中的Feed-Forward Networks
> - Layer Normalization的原理和重要性
> - Transformer模型中的残差连接
> - 编码器和解码器的结构差异
>
> **第五章：Transformer模型原理剖析（3）**
>
> - Transformer的训练策略和优化方法
> - 参数初始化和学习率调度
> - Transformer模型的正则化技术
> - Attention机制的变种和改进
> - Greedy Decoding, Beam-search
> - Top-K Sampling, Top-p Sampling
> - Transformer源码解读
>
> **第六章：Transformer模型全量微调和高效微调**
>
> - 全量微调与高效微调的区别
> - Transformer模型微调的常见策略
> - 选择合适的微调任务和数据集
> - 微调中的挑战和最佳实践
> - 评估微调效果的标准和工具
>
> **第七章：【项目实战1】大模型PEFT微调项目**
>
> - PEFT的安装
> - PEFT的使用说明，核心模块讲解
> - 指令数据准备和预处理的技巧
> - 实施微调的详细步骤
> - 微调项目的性能评估和分析
>
> **第八章：GPT模型家族剖析**
>
> - GPT系列模型的发展历程
> - GP1到GPT4，GPT3模型剖析
> - GPT代码解读
> - InstructGPT模型剖析
> - Zero-shot Prompting
> - Few-shot Prompting
> - GPT模型的局限性和挑战
>
> **第九章：LLaMA家族模型剖析**
>
> - LLaMA模型的特点和技术创新
> - LLaMA模型的原理剖析
> - LLaMA源码解读
> - LLaMA与其他大模型的对比
> - LLaMA模型的训练和微调策略
> - 面对LLaMA模型的未来发展方向
>
> **第十章：ChatGLM家族模型剖析**
>
> - ChatGLM的架构和设计理念
> - ChatGLM模型解读
> - ChatGLM1到ChatGLM3的技术迭代
> - ChatGLM模型的优势和应用领域
> - ChatGLM模型微调和部署的实践指南
> - ChatGLM模型的评估和性能优化
>
> **第十一章：Baichuan家族模型剖析**
>
> - Baichuan模型的概述和核心技术
> - Baichuan原理剖析和源码解读
> - Baichuan模型与其他模型的比较
> - Baichuan模型在特定任务上的应用
> - 微调Baichuan模型的策略和技巧
> - Baichuan模型的局限
>
> ## **第二阶段：大模型指令微调之-LoRA**
>
> **第十二章：指令微调基础**
>
> - 指令微调的定义与应用背景
> - 指令微调与传统微调的对比
> - 指令微调在大模型中的重要性
> - 指令微调流程概览
> - 指令微调的挑战与策略
>
> **第十三章：必要矩阵知识**
>
> - 矩阵和向量的基本概念
> - 矩阵运算与性质
> - 特征值和特征向量
> - 矩阵分解（SVD）技术简介
> - 矩阵在LoRA算法中的应用
>
> **第十四章：LoRA算法剖析**
>
> - LoRA算法的原理与动机
> - Lora中的Low-rank假设
> - LoRA的关键技术组件
> - LoRA算法的实现步骤
> - LoRA算法的优化与调试
> - LoRA算法源码解读
>
> **第十五章：指令数据搜集和生成**
>
> - 指令数据的重要性与来源
> - 自动化和手动搜集指令数据的方法
> - 指令数据的预处理和标准化
> - 生成高质量指令数据的技巧
> - 指令数据集的维护与更新
> - 指令数据的人工质量评估与自动质量评估
>
> **第十六章：【项目实战2】Alpaca微调大模型**
>
> - Alpaca微调项目的设计与目标
> - 准备Alpaca微调所需的指令数据
> - 实施Alpaca微调的详细步骤
> - 评估Alpaca微调效果的方法
> - 分析与解决Alpaca微调中遇到的问题
> - 解读Alpaca项目源码
>
> **第十七章：AdaLoRA算法剖析**
>
> - AdaLoRA与LoRa的比较
> - 动态改变矩阵权重的意义
> - SVD与AdaLoRA
> - 训练AdaLoRA
> - AdaLoRA源码解读
> - AdaLoRA案例讲解
>
> **第十八章：【项目实战3】Vicuna微调大模型**
>
> - Vicuna微调项目的背景与应用场景
> - ShareGPT数据收集
> - Vicuna微调的实施流程和技术细节
> - Vicuna微调效果的评估与分析
> - 基于Vicuna微调项目的经验总结与展望
>
> ## **第三阶段：大模型指令微调之-Quantization**
>
> **第十九章：模型Quantization基础**
>
> - Quantization在深度学习中的作用与原理
> - 常见的Quantization技术及其分类
> - 模型Quantization对性能和精度的影响
> - Quantization的实践步骤和工具
> - 模型Quantization的挑战与解决策略
>
> **第二十章：QLoRA算法剖析**
>
> - QLoRA算法的定义和背景
> - QLoRA与LoRA的关键区别和改进
> - QLoRA算法的详细实现过程
> - 4bit NormalFloat, double quantization
> - QLoRA算法的优化和调试技巧
> - QLoRA源码解读
>
> **第二十一章：【项目实战4】QLoRA微调LLaMA大模型**
>
> - 技术方案的设计
> - 收集和预处理指令数据
> - 基于PEFT进行QLora大模型微调
> - 评估QLoRA微调之后的效果
> - 分析QLoRA微调过程中遇到的问题及其解决方案
>
> **第二十二章：模型Compression技术**
>
> - 模型压缩的必要性和技术背景
> - 常见的模型压缩方法概述
> - 模型压缩与Quantization的关系
> - 实施模型压缩的步骤和注意事项
> - 模型压缩技术的最新研究进展
>
> **第二十三章：模型蒸馏技术探索**
>
> - 模型蒸馏的基本概念和工作原理
> - 模型蒸馏在模型优化中的应用
> - 不同蒸馏技术的比较和选择
> - 实施模型蒸馏的具体方法
> - 模型蒸馏技术面临的挑战及其解决策略
>
> **第二十四章：ZeroQuant算法剖析**
>
> - ZeroQuant算法的基本原理和应用背景
> - ZeroQuant在模型Quantization中的创新点
> - 实现ZeroQuant的关键步骤和技术要求
> - ZeroQuant源码解读
> - ZeroQuant技术的局限性和未来方向
>
> **第二十五章：SmoothQuant算法剖析**
>
> - SmoothQuant算法的设计理念和核心技术
> - SmoothQuant与传统Quantization方法的区别
> - 实施SmoothQuant算法的具体流程
> - SmoothQuant源码解读
> - SmoothQuant面临的技术挑战和改进路径
>
> ## **第四阶段：大模型对齐之-RLHF**
>
> **第二十六章：RLHF算法概述**
>
> - RLHF的起源和背景
> - RLHF在人工智能中的作用和重要性
> - 强化学习与人类反馈：结合的优势
> - RLHF的主要应用领域和案例研究
> - 从InstructGPT到GPT4
>
> **第二十七章：人类反馈的集成**
>
> - 人类反馈在强化学习中的角色
> - 不同形式的人类反馈：标注、偏好、指导
> - 从人类反馈中学习：方法和策略
> - 人类反馈数据的收集和处理
> - 人类反馈强化学习的挑战和解决方案
>
> **第二十八章：PPO算法概述**
>
> - PPO的起源和动机
> - PPO与其他策略梯度方法的对比
> - 算法核心概念和原理
> - PPO的优势和局限性
> - PPO的应用领域和案例
>
> **第二十九章：强化学习和数据基础**
>
> - 强化学习基本概念介绍
> - 数据在强化学习中的作用和重要性
> - 状态、动作和奖励的数据结构
> - 数据收集、处理和利用的方法
> - 使用模拟环境进行数据生成和测试
>
> **第三十章：策略优化基础**
>
> - 策略梯度方法简介
> - 优势函数和回报
> - 基线的概念和作用
> - 累积回报与折扣回报
> - 探索与利用的权衡
>
> **第三十一章：PPO核心技术细节**
>
> - 目标函数和KL散度
> - 裁剪目标函数的原理
> - 多次迭代优化策略
> - 广义优势估计（GAE）
> - 重要性采样和策略更新
>
> **第三十二章：基于开源大模型从零实现PPO算法**
>
> - 构建神经网络模型
> - 实现PPO的优化循环
> - 自适应学习率调整
> - 调试和性能分析技巧
> - 评估对齐之后的大模型
>
> **第三十三章：高级PPO技术和强化学习进阶**
>
> - PPO变体和改进策略
> - 处理高维输入和模型泛化
> - 多智能体环境中的PPO应用
> - 强化学习中的迁移学习和多任务学习
> - 强化学习中的安全性和可解释性
>
> **第三十四章：【项目实战5】RLHF医疗大模型微调**
>
> - 项目需求分析和技术方案设计
> - 环境设置和任务定义
> - 对齐数据的收集和预处理
> - 实现PPO训练流程
> - 结果分析和性能优化
>
> ## **第五阶段：大模型对齐之-DPO**
>
> **第三十五章：DPO算法概述**
>
> - DPO（Direct Preference Optimization）介绍
> - 与PPO算法对比
> - DPO的应用场景和重要性
> - 基本原理和工作机制
> - DPO算法的优势和挑战
>
> **第三十六章：排序和偏好的基础**
>
> - 偏好与排序问题在AI中的角色
> - 数据表示：成对比较和偏好矩阵
> - 偏好学习的挑战
> - 排序和偏好预测的评估指标
> - 经典偏好学习算法概览
>
> **第三十七章：DPO核心技术细节**
>
> - 偏好建模的数学框架
> - 直接与间接偏好优化的对比
> - DPO中的关键算法组件
> - 成对比较数据的处理方法
> - DPO的损失函数和优化策略
>
> **第三十八章：DPO算法的从零实现**
>
> - 数据整理与预处理
> - 构建偏好学习模型的步骤
> - 使用Python实现基础DPO模型
> - 在benchmark上测试DPO性能
> - DPO的优势和缺点
>
> **第三十九章：【项目实战6】DPO在推荐系统中的应用**
>
> - 推荐系统中的偏好学习
> - 设计DPO驱动的推荐算法
> - 处理实时用户反馈
> - 实施DPO进行推荐模型微调
> - 评估推荐系统的性能
>
> **第四十章：高级DPO技术**
>
> - 多任务学习与DPO的结合
> - DPO在非监督学习中的应用
> - 深度学习方法与DPO
> - 交互式偏好学习
> - DPO技术的变种
>
> ## **第六阶段：大模型其他微调技术**
>
> **第四十一章：Prefix Tuning算法剖析**
>
> - Prefix Tuning的基本原理
> - 实现Prefix Tuning的关键步骤
> - Prefix Tuning源码解读
> - Prefix Tuning与其他微调方法的比较
> - 在NLP任务中应用Prefix Tuning的案例
> - Prefix Tuning的局限性和挑战
>
> **第四十二章：Adaptor Tuning算法剖析**
>
> - Adaptor Tuning的基本原理
> - 如何在大模型中插入Adaptor层
> - Adaptor Tuning的优点和应用场景
> - Adaptor Tuning源码解读
> - 实际案例：Adaptor Tuning在分类任务中的应用
> - Adaptor Tuning的效率和扩展性问题
>
> **第四十三章：Flash Attention算法剖析**
>
> - Flash Attention的设计思想和算法原理
> - 优化Transformer模型中的注意力机制
> - Flash Attention在提升处理速度和效率上的作用
> - 应用Flash Attention改进大模型的案例分析
> - Flash Attention的实现挑战和解决方案
>
> **第四十四章：Flash Attention 2算法剖析**
>
> - 介绍Flash Attention 2与前版本的区别
> - 深入探讨Flash Attention 2的技术改进点
> - Flash Attention 2在复杂任务处理中的应用示例
> - 评估Flash Attention 2的性能和适用范围
> - Flash Attention 2的实现细节和调优建议
>
> **第四十五章：Kahneman-Tversky Optimization (KTO) 算法剖析**
>
> - KTO算法背景和理论基础
> - Kahneman-Tversky优化在微调中的应用
> - 实施KTO的关键技术步骤
> - KTO在提高决策质量中的角色
> - KTO应用案例和性能分析
>
> **第四十六章：【项目实战7】QLoRA+Flash Attention微调大模型**
>
> - 结合QLoRA和Flash Attention的微调策略
> - 任务选取和数据准备
> - 微调流程详解：从预处理到模型评估
> - 分析微调后模型的性能改进
> - 面临的挑战及解决方案分享
>
> ## **第七阶段：大模型增量学习**
>
> **第四十七章：大模型增量学习概述**
>
> - 增量学习（Continual learning）的重要性
> - 与传统从零训练的对比
> - 增量学习的应用场景
> - 任务选取和数据准备
> - 微调流程详解：从预处理到模型评估
>
> **第四十八章：增量学习与灾难性遗忘**
>
> - 什么是灾难性遗忘
> - 解决灾难性遗忘的思路
> - 正则化、动态网络架构、元学习
> - 通用数据与垂直数据的混合训练
> - 数据中的信息分析
> - 调整学习率
>
> **第四十九章：增量学习中的高级主题**
>
> - 增量学习在大规模数据集上的应用
> - 多模态与跨领域增量学习
> - 自适应学习和在线学习技术
> - 强化学习与增量学习的结合
> - 未来增量学习的发展方向



## transformers

> 【Huggingface Transformers】保姆级使用教程：https://zhuanlan.zhihu.com/p/448852278

- transformers安装：`pip install transformers[sentencepiece]`



## cuda和pytorch版本统一

- 参考：https://pytorch.org/get-started/previous-versions/

- 对于CUDA 11.6来说：

  ```
  pip install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1
  ```

- 

## Qwen1.5

> Qwen1.5开源实践：https://zhuanlan.zhihu.com/p/681662566

- 魔搭社区模型下载：

  ```python
  from modelscope import snapshot_download
  model_dir = snapshot_download('qwen/Qwen1.5-7B-Chat')
  ```





## 怎么下载大模型

- python版本选择 3.10及以上
- 安装huggingface_hub：`pip install -U huggingface_hub`
- 然后下载：`huggingface-cli download --resume-download Qwen/Qwen2.5-14B-Instruct --local-dir Qwen2.5-14B-Instruct`
- 移动到服务器：`scp -r -P 10022 Qwen2.5-14B-Instruct/ root@10.5.32.171:/data/model_dir/qwen/`

参考：[国内下载huggingface的模型和使用](https://zhuanlan.zhihu.com/p/677862503)、[huggingface-cli下载数据（含国内镜像源方法）](https://blog.csdn.net/lanlinjnc/article/details/136709225)