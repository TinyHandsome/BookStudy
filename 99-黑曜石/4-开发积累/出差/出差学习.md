# 出差学习

## 20240315 智慧芽

1. 基础架构是Lamma，所有数据从零开始训练
2. 训练模型的四步：对common数据的pretrain（上千亿的token，BPE分词token，前缀分词的树，对多语言统一处理），post pretrain（强化知识，主要数据是专利数据、文献数据、垂直数据等等，知乎的数据就扔掉了；增加长度4k-16k，rope算法做的加长，4k是输入2k输出2k），SFT（更加focus on论文上，客服相关的知识），RLHF（10w对，模型输出的结果的意图和表达方式跟人更加相近）
3. 外部数据，搭了一个数据处理平台（天机），自动爬虫的功能（百度、知乎、wikki），对数据加工，然后向量化，对结果打分，选出可用的数据。
4. 内部数据，数据来源打标签，在哪个阶段用的，在哪一轮训练用了，这样就可以可追溯。
5. RAG系统，ebedding、重拍、关键词token。
6. 有一个统一信息抽取的框架，业务的知识，生成一个对应的prompts
7. SFT：专家纯人工，GPT生成+专家修改（两个人双标，一致才入库，只需要判断对不对）。30%专家，70%GPT。5w多条QA对。数据质量一定要非常的高。文本重合度低，文本要多样。
8. 标注消耗很多，但是没有透露实际的数量。
9. 拉下来的数据，embedding，分类干净还是非干净。（自己训练的bert分类模型）
10. 表格处理为markdown
11. 200 A800 一个月，训练一轮，租用的阿里云的。
12. Agent：拆解意图，层层选择API，核心是memory。Agent的核心是根据意图工程和CoT做选择。
13. RAG很多文本怎么解决：①embedding的准确率更高（符合业务的、对应的知识rank应该更多），②长文本理解更强。**意图识别区获取知识的分组。**
14. 加了权限，意图识别中。意图要过于敏感，意图专门有一个大模型。**MOE**到专业大模型、通用大模型。
15. 效果评价：考试、特定任务的测试集。自己标记的数据自己出题。
16. 多语言：扔给它的文本是多语言的，它的回答就是多语言的。自己学或者前后加翻译器。
17. 天机相当于AI中台，很多开发的小模型都部署上去了。相当于流程处理和API部署平台、流程搭建。