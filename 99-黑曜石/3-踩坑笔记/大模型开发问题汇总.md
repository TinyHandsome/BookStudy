# 大模型开发问题汇总

## 图+文->图

1. glm4v-9b多用户同时推理报错：

   ```
   RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
   ```

   1. 【失败】方法：设置 `do_sample=False`
   
      结果：新的报错如下
   
      ```
      RuntimeError: CUDA error: device-side assert triggered
      ```
      
       > [【NLP】关于参数do_sample的解释](https://blog.csdn.net/weixin_43941438/article/details/140584648)
       >
       > 在自然语言处理（NLP）领域，特别是在使用神经网络模型进行文本生成时，do_sample是一个常见的参数，用于控制模型生成文本的方式。具体来说，do_sample参数决定**模型是否采用随机采样**（sampling）的方式来生成下一个词，还是仅仅选择最有可能的词。
       >
       > 当 do_sample=False 时，模型将采用贪心搜索（Greedy Search）策略。这意味着在每一个时间步，模型都会选择具有最高概率的下一个词。这种方法简单快速，但可能会导致生成的文本过于保守，缺乏多样性，因为总是选择最可能的选项，**可能会错过一些虽然概率较低但能产生更有趣或更合理文本的词**。
       > 当 do_sample=True 时，模型会根据词的概率分布进行随机采样。在每个时间步，下一个词的选择是基于其预测概率的随机过程。这增加了生成文本的多样性和创造性，因为即使概率较低的词也有机会被选中。为了控制这种随机性，通常还会配合使用其他参数，如temperature、top_k和top_p等，来调整采样的范围和概率分布。
       >
       > ==temperature:arrow_up: -> 随机性:arrow_up:== temperature参数则会影响采样时概率分布的形状，从而影响生成文本的多样性。较高的temperature值会使分布更加均匀，增加随机性；较低的temperature值会使分布更加尖锐，减少随机性，更倾向于选择高概率的词。
   
   2. 方法：设置 `torch_dtype=torch.float16`
   
      结果：不行，还是报错
   
   3. 方法：设置 `low_cpu_mem_usage=False`
   
      结果：不行，还是报错
   
   4. 方法：[不用transformers，换为vllm](https://github.com/xorbitsai/inference/issues/1722)
   
      
