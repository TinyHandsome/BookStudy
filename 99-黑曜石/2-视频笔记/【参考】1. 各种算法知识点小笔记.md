# 小笔记

- [机器学习特征选择的方法](https://www.cnblogs.com/bonelee/p/8632866.html)
- [sklearn聚类使用](https://www.cnblogs.com/little-horse/p/7367407.html)
- [典型推荐算法总结](https://blog.csdn.net/u011095110/article/details/84403564)
- [机器学习算法总结](https://www.cnblogs.com/jiangxinyang/p/9217424.html)
- [浅谈机器学习方法](https://www.cnblogs.com/flippedkiki/p/7209076.html?utm_source=itdadao&utm_medium=referral)
- [Bagging和Boosting 概念及区别](https://www.cnblogs.com/gczr/p/7097442.html)
- [机器学习面试问题大概梳理](https://www.cnblogs.com/gczr/p/6829176.html)
- [Keras学习率调整](https://www.cnblogs.com/nxf-rabbit75/p/10564888.html)
- [网格搜索参数详解](https://blog.csdn.net/CherDW/article/details/54970366)
- [多标签分类、多任务分类、多输出回归概念](https://blog.csdn.net/zb1165048017/article/details/77882600)
- [详解机器学习中的熵、条件熵、相对熵和交叉熵](https://www.cnblogs.com/kyrieng/p/8694705.html)
- [集成学习调参]( https://blog.csdn.net/kylin_learn/article/details/82192045 )
- [详解机器学习中的熵、联合熵、条件熵、相对熵和交叉熵](https://www.cnblogs.com/zzdbullet/p/10075347.html)
- [偏差、方差和误差](https://www.zhihu.com/question/27068705)

GBDT、XGBoost、lightBoost
- [XGBoost](https://www.hrwhisper.me/machine-learning-xgboost/)
- [GBDT梯度提升树](https://blog.csdn.net/weixin_42933718/article/details/88421574#GBDT_2)
- [机器学习-树模型理论（GDBT，xgboost，lightBoost，随机森林）](https://www.cnblogs.com/onemorepoint/p/9799124.html)
- [提升树、GBDT与XGBoost](https://mp.weixin.qq.com/s/ePHvIADfBBhW8hlsmvidrQ)

机器学习实战参考资料：

- [随机森林代码与调参](https://www.jianshu.com/p/5354f2a42a73)，[随机森林的参数说明](https://www.cnblogs.com/gczr/p/7141712.html)
- [逻辑回归的参数说明](https://blog.csdn.net/sun_shengyun/article/details/53811483)，[逻辑回归调参](https://www.jianshu.com/p/99ceb640efc5)，[逻辑回归代码与调参](https://www.cnblogs.com/onemorepoint/p/9486998.html?utm_source=debugrun&utm_medium=referral)
- [sklearn常见分类器(二分类模板)](https://www.cnblogs.com/caiyishuai/p/11385825.html)
- [python sklearn常用分类算法模型的调用](https://my.oschina.net/tantexian/blog/1919888)

Java面试算法必须掌握：

- [Java面试-排序算法](https://blog.csdn.net/weixin_41835916/article/details/81661314)
- [Java面试-查找算法](https://blog.csdn.net/babylorin/article/details/67638156)
- [排序算法整合（冒泡，快速，希尔，拓扑，归并）](https://blog.csdn.net/onceing/article/details/99838520)
- [**动画图解：十大经典排序算法动画与解析**](https://blog.csdn.net/kexuanxiu1163/article/details/103051357)

## 1. 机器学习原理和公式推导：

- [决策树](https://blog.csdn.net/qq_24519677/article/details/82084718)

### 1.1 Adaboost

- [Adaboost公式推导1](https://www.jianshu.com/p/0d850d85dcbd)、[Adaboost公式推导2](https://zhuanlan.zhihu.com/p/62037189)
- [Adaboost原理1](https://www.cnblogs.com/ScorpioLu/p/8295990.html)、[Adaboost原理2](https://blog.csdn.net/qq_24519677/article/details/81910112)
- [Adaboost改进](https://www.jianshu.com/p/0bfce1806235)

### 1.2 朴素贝叶斯

- [小白之通俗易懂的贝叶斯定理](https://mp.weixin.qq.com/s/nxaDurqi9D61wwpQiTDudQ)
- [贝叶斯和朴素贝叶斯是啥](https://www.cnblogs.com/chenqionghe/p/12598786.html)
- [带你理解朴素贝叶斯算法](https://zhuanlan.zhihu.com/p/26262151)
- [机器学习之贝叶斯](https://blog.csdn.net/weixin_42180810/article/details/81278326)
- [朴素贝叶斯为什么朴素？](https://zhuanlan.zhihu.com/p/150641206)

### 1.3 PCA

- [PCA详解](https://zhuanlan.zhihu.com/p/77151308)

## 2. Pandas技巧

### 2.1 [dropna](https://blog.csdn.net/weixin_40283816/article/details/84304055)

```python
data.dropna(how = 'all')    # 传入这个参数后将只丢弃全为缺失值的那些行
data.dropna(axis = 1)       # 丢弃有缺失值的列（一般不会这么做，这样会删掉一个特征）
data.dropna(axis=1,how="all")   # 丢弃全为缺失值的那些列
data.dropna(axis=0,subset = ["Age", "Sex"])   # 丢弃‘Age’和‘Sex’这两列中有缺失值的行   
```

### 2.2 apply返回多列

- [pandas 的apply返回多列，并赋值](https://www.jianshu.com/p/1eaf89990ce7)

- 方法1：使用apply 的参数result_type 来处理

  ```python
  def formatrow(row):
      a = row["a"] + str(row["cnt"])
      b = str(row["cnt"]) + row["a"]
      return a, b 
  
  df_tmp[["fomat1", "format2"]] = df_tmp.apply(formatrow, axis=1, result_type="expand")
  df_tmp
  ```

- 方法2：使用zip打包返回结果来处理

  ```python
  df_tmp["fomat1-1"], df_tmp["format2-2"] = zip(*df_tmp.apply(formatrow, axis=1))
  ```

## 3. Tkinter资料

### 3.1 组件功能详解

- [Entry组件详解](https://blog.csdn.net/qq_41556318/article/details/85108328)
- [基本属性详解，鼠标样式、三维效果等](https://www.cnblogs.com/wangdianchao/p/11521825.html)

### 3.2 技巧

- [TK官方文档](https://tkdocs.com/tutorial/index.html)
- [tk.root窗口最前等设置](https://www.dazhuanlan.com/2019/09/05/1e67bcde48d4/?__cf_chl_jschl_tk__=5f920d84c9495a55f8a2f17de7690186207a43c1-1601347614-0-ARY3T754C5CNpkyoGrwYZPJJkdk3BilNxdOIeNED4BEC0zChdzqFxQF4xruKDJtWMoTP-446-LsQJbJrvXFSXSoSg1dNlDookJax3so3zRxf4jVWhAhw4DMAJRZkPsoK4W6QGeqi1wxzWW1L7C00XZIFPskmEx05avcFOs169suZKtn0ow9zOaPmxnyCFrhGvb9kQ47cRH9lyqi86quGLxQCdwchBCQTButILMGlBghhaoIp6Jy10djOgt3uzEjmgk6Q5__DSyxwnxc9QCglxCKPs88OeIIjdrJ4N38nzofpvz8_S8ocHxYC00GNWdo73w)
- [用tkinter.pack设计复杂界面布局](https://blog.csdn.net/superfanstoprogram/article/details/83713196)

## 4. Python资料

### 4.1 lambda

- [for循环中的lambda与闭包](https://www.jianshu.com/p/84f3e0f4d218)

### 4.2 pip

#### 4.2.1 python pip安装模块加速的方法

- [加速！飞一般的感觉！](https://blog.csdn.net/joonchen111/article/details/52263296)

## 5. Numpy

### 5.1 `np.random.permutation`

- 生成随机序列，即

![在这里插入图片描述](https://img-blog.csdnimg.cn/2020090411114134.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzIxNTc5MDQ1,size_16,color_FFFFFF,t_70#pic_center)

### 5.2 `np.split`

- 把一个list均匀划分，如果不能均等划分就会报错

### 5.3 `np.array_split`

- 相较于`np.split`，可以不均等划分

## 6. Jupyter

### 6.1 用ppt的形式打开jupyter

- [jupyter制作PPT式的演示文稿，需要下载安装一些东西，别忘了](https://blog.csdn.net/blmoistawinde/article/details/85009603)

- [上面的链接是教程，但是需要安装的东西，及方法看这个链接（要安装RISE并配置）](https://blog.csdn.net/gx19862005/article/details/60141711)

## 7. PyCharm | IDEA

### 7.1 PyCharm注册码/激活码/破解教程

- [PyCharm2019 激活码](https://blog.csdn.net/u014044812/article/details/78727496)
- [**两步解决jetbrains-agent-latest.zip不能拖入工作区**](https://blog.csdn.net/qq_21579045/article/details/108804218)

### 7.2 记一次IntelliJ IDEA中文乱码问题

- 问题描述：输出控制台中文乱码，反正就是各种百度解决不了
- 问题解决：https://blog.csdn.net/m0_37893932/article/details/78280663
- 解决方案：我用的是上述连接里面博主讲的，第三种，修改工程目录中的encodings.xml文件，只要看到是编码的无论是gbk还是什么，都改成utf-8，我就是把一个gbk修改成了utf-8就解决了，你说气人不气人。

## 机器学习小知识点

### 评价指标

- [宏平均和微平均的对比](https://blog.csdn.net/Candy_GL/article/details/83059217)
- [sklearn中 F1-micro 与 F1-macro区别和计算原理](https://blog.csdn.net/lyb3b3b/article/details/84819931)
- [precision和accuracy的区别](https://blog.csdn.net/Du_Shuang/article/details/84073584)

### 数据不均衡

- [正负样本不均衡的解决办法](https://blog.csdn.net/jemila/article/details/77992967)
- [class_weight和sample_weight区别]( https://blog.csdn.net/weixin_40755306/article/details/82290033 )

### 支持向量机

- Support Vector Machine, SVM

- [SVM时间复杂度](https://blog.csdn.net/iteye_12567/article/details/81921956)

  ```python
  sklearn.svm.SVC(C=1.0, kernel=’rbf’, degree=3, gamma=’auto_deprecated’, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=’ovr’, random_state=None)
  ```

- **C**：惩罚系数，理解为调节优化方向中两个指标（间隔大小，分类准确度）偏好的权重，即对误差的宽容度，**C越高，说明越不能容忍出现误差,容易过拟合，C越小，容易欠拟合，C过大或过小，泛化能力变差**。

- **gamma**：是选择RBF函数作为kernel后，该函数自带的一个参数。隐含地决定了数据映射到新的特征空间后的分布，**gamma越大，支持向量越少，gamma值越小，支持向量越多**。支持向量的个数影响训练与预测的速度。

### 聚类

#### 1. 均值漂移（MeanShift）

- 参考链接：[通俗理解Meanshift均值漂移算法](https://www.cnblogs.com/lowbi/p/10733733.html)，[Meanshift Algorithm](http://www.chioka.in/meanshift-algorithm-for-the-rest-of-us-python/)

- 算法复杂度$O(kn^2)$，其中n是数据点的数量，k是Meanshift的迭代次数。

- **概述**

  Mean-shift（均值迁移）的基本思想：在数据集中选定一个点，然后以这个点为圆心，r为半径，画一个圆(二维下是圆)，求出这个点到所有点的向量的平均值，而圆心与向量均值的和为新的圆心，然后迭代此过程，直到满足一点的条件结束。

  后来在此基础上加入了**核函数和权重系数**，使得Mean-shift 算法开始流行起来。目前它在**聚类、图像平滑、分割、跟踪**等方面有着广泛的应用。

- **图解过程**

  第一张图有一个子中心点,她向四周最近的点开始寻找,找到圆心与向量均值的和为新的圆心,然后依次循环,直到满足条件,则不会再寻找其他圆心点。

  ![img](https://img2018.cnblogs.com/blog/1567636/201904/1567636-20190419074346180-717338129.png)

- **Mean-shift 算法函数**

  - 核心函数：sklearn.cluster.MeanShift(核函数：RBF核函数)

    由上图可知，圆心(或种子)的确定和半径(或带宽)的选择，是影响算法效率的两个主要因素。所以在sklearn.cluster.MeanShift中重点说明了这两个参数的设定问题。

  - 主要参数

    - bandwidth：半径(或带宽)，float型。如果没有给出，则使用sklearn.cluster.estimate_bandwidth计算出半径(带宽)。（可选）
    - seeds：圆心（或种子），数组类型，即初始化的圆心。（可选）
    - bin_seeding：布尔值。如果为真，初始内核位置不是所有点的位置，而是点的离散版本的位置，其中点被分类到其粗糙度对应于带宽的网格上。将此选项设置为True将加速算法，因为较少的种子将被初始化。默认值：False.如果种子参数(seeds)不为None则忽略。

  - 主要属性

    - cluster_centers_：数组类型。计算出的聚类中心的坐标。
    - labels_：数组类型。每个数据点的分类标签。

- **核函数**

  核函数只是用来计算映射到高维空间之后的内积的一种简便方法，目的为让低维的不可分数据变成高维可分。利用核函数，可以忽略映射关系，直接在低维空间中完成计算。

- **引入核函数的偏移均值**

  在均值漂移中引入核函数的概念，能够使计算中**距离中心的点具有更大的权值**，反映距离越短，权值越大的特性。

