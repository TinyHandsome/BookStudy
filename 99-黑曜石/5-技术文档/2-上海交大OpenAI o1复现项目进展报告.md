[TOC]

> **参考：**上交大发布首个OpenAI o1复现项目进展报告，满满的经验洞察：https://mp.weixin.qq.com/s/ZO_Rv98OakPuBaZl9Tw5VA
>
> **感受：**
>
> - 旅程学习是o1成功的关键。o1 的成功强调了 AI 系统不仅要学习结果，还要学习完整的科学探索过程，包括试错的重要性。
> - 构建完整的思维过程非常重要，特别是要把日常思考中发散的认知路径转为显式的认知。
> - 高质量数据需要包含人类容易省略的常识性知识。

## 前言

在人工智能领域掀起巨浪的 OpenAI o1 模型发布三周后，一支由高校年轻研究者组成的团队今天发布了题为 "o1 Replication Journey: A Strategic Progress Report (o1 探索之旅：战略进展报告)" 的研究进展报告。

研究者认为 **旅程学习** 是o1驱动成功的关键技术。通过 327 条训练样本，鼓励模型学会反思、纠错、回溯，其在复杂数学题目上 **表现绝对性能就超过了传统监督学习8%以上，相对性能提升超过20%**。

团队提出的模型在同一道数学题上，与 OpenAI 的 o1-preview （答对）及 GPT-4o（答错）的比较实例，证明旅程学习不断试错、反思、自我纠正的能力在复杂推理任务场景上非常关键。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicyGrm6YH575tkcscxxKbyH8ulZyETFLdcVUE2OFRZoN6nVibf0auAlEuuDqqHiaiaH6kqvb4QFXY0Dg/640)

## 捷径学习

从 "捷径学习" 到 "旅程学习" 的范式转变。这是一个用于推理任务的搜索树。对于数学问题解决任务，根节点代表初始问题，而叶节点则是最终结论。绿色节点表示正确答案，红色节点表示错误答案。传统上，学习主要集中在对直接从根到叶的捷径路径进行监督训练。然而，本研究探索了对整个探索路径进行监督学习，这包括了试错和纠正的过程。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicyGrm6YH575tkcscxxKbyHpHyT2TL27DdsmJnb4FAvFUvOdAlFMSWssqhkUwibt7tibYAyXLay5ibsA/640)

多数现有的机器学习或大模型训练方法（如监督式微调）都可以被归类为 **"捷径学习" (Shortcut Learning)**，即模型学习到达正确答案的直接路径。这种传统范式虽然在特定、明确定义的任务中可能有效，但在面对复杂、动态和开放性问题时显示出明显的局限性。捷径学习具有以下几个关键特征：

1. **注重快速结果**：强调在短时间内达到特定的性能指标或完成特定任务。
2. **高度依赖数据**：性能改进通常依赖于增加训练数据量，而非改进学习算法本身。
3. **泛化能力有限**：在训练数据分布之外的场景中，性能可能会急剧下降。
4. **缺乏自我纠正能力**：这些系统通常缺乏识别和纠正自身错误的能力。

尽管捷径学习推动了人工智能的许多进步，但它难以产生真正智能和可靠的人工智能系统，无法应对现实世界挑战的复杂性。随着我们追求更高级形式的人工智能甚至超级智能，这种方法的局限性变得越来越明显。

## 旅程学习

认识到这些缺点，作者提出了一种名为 **"旅程学习"（Journey Learning)** 的新范式。旅程学习旨在使人工智能系统能够通过学习、反思、回溯和适应不断进步，就像人类一样，从而展现出更高水平的智能。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicyGrm6YH575tkcscxxKbyHMta6cgbYmgTibl6AAweDCSJcNLvkPfopeRoFCvRsqn9D7Pe1wGKmesg/640)

## 复现o1过程中的几个关键问题

### Q1：o1的思维链是什么样子的？

观测：

- 随着难度的增加，响应长度（包括标记数和行数）往往成比例增长。这表明更高难度的问题涉及更多的推理步骤。
- "consider"、"if" 和 "possible" 等关键词经常出现，通常表示推理过程中的分支，考虑多条路径。
- 像 "wait" 和 "Alternatively" 这样的关键词是模型能够进行反思和自我纠正的重要指标。这表明模型具有更深入的理解和更细致的推理方法，因为模型不仅仅是遵循线性路径，还能够基于反思重新考虑和完善其方法。

经过这些探索，作者确定需要构建的长思维数据应具有以下特征：

- **迭代式问题解决**：模型首先定义函数，然后逐步探索相关表达式，将复杂方程分解为更简单的组成部分，反映了一种结构化和有条理的方法。 
- **关键思维指标**：使用 "Therefore" 表示结论，"Alternatively" 探索不同路径，"Wait" 表示反思，以及 "Let me compute" 过渡到计算，突出了模型的推理阶段。 
- **递归和反思方法**：模型经常重新评估和验证中间结果，使用递归结构确保一致性，这在严谨的数学推理中很典型。 
- **假设探索**：模型测试不同的假设，随着获得更多信息而调整其方法，展示了推理过程中的灵活性。
- **结论和验证**：最后，模型解方程并验证结果，强调在完成之前验证结论的重要性。 

### Q2：长思维是如何工作的？

o1 长思维（Long thought）方法的显著成功可以归因于在上述中介绍的旅程学习。与传统的捷径学习不同，旅程学习允许模型探索整个决策轨迹，模仿人类的问题解决过程。这种全面的探索使 o1 能够**考虑多种解决方案路径，从错误中学习，并理解完整的问题解决过程**。

这种方法培养了对问题领域更深入的理解，不仅仅是知道正确答案，而是理解为什么以及如何得出答案。

### Q3：如何构建长思维？

- **尝试 1：基于 LLM 和奖励的树搜索**：根据在 Q1 中对长思维的观察，其最显著的特征是在推理产生错误时或遇到冗余的推理步骤时尝试反思和回溯。
- **尝试 2：提议-批评循环**：尝试 1 通过基于预定义规则在树上执行搜索来构建长思维，但这限制了回溯和反思等行为的自由度。团队构建了一个提议-批评循环，其中为模型预定义了一些可能的行为（即继续、回溯、反思、终止），并让模型自身选择行为来构建推理树。如果树没有达到最终答案，可以将这个负面信号告知模型，引导它反思和纠正其方法。
- **尝试 3：多智能体方法**：基于推理树构建长思维存在几个挑战，包括存在许多冗余的无效节点，以及存在不依赖于反思行为的推理步骤，从而引起构建的长思维逻辑不一致。为解决这个问题，团队设计了一个利用**多智能体辩论**的算法，其中一个智能体充当策略模型，持续推理，而另一个智能体充当评论模型，指示策略模型是否应该继续当前推理或执行回溯等行为。两个智能体进行持续对话，在找到正确答案时自然构建长思维数据集。
- **尝试 4：完整的人类思维过程注释**：当人类处理推理问题时，他们通常不会不断地向前推理直到解决问题或失败；相反，他们在无法继续时会反思、回溯和重写推理。这种行为与长思维的特征高度一致。因此，可以忠实且全面地记录人类解决推理任务的过程，从而产生高质量的长思维。

### Q4：如何构建奖励模型

使用奖励模型的第一步是定义粒度。团队的目标不仅仅是关注最终结果，而是专门提高 LLMs 在反思、回溯和相关认知过程方面的能力。因此，团队将评估粒度定义在**步骤层面**。

实现奖励模型的过程可以使用开源模型或是调用闭源模型的api。结果表明 o1-mini 在不同数据集上表现最佳，证明其是一个良好的奖励模型。

### Q5：如何构建 on-policy 推理树？

构建推理树需要一个**能够执行单步推理的策略模型**。给定一个问题及其相应的最终答案，策略模型从问题作为根节点开始，不断向树中添加新节点。它首先生成 w 个可能的第一步推理步骤作为根节点的子节点。然后，它迭代地进行前向推理，为每个当前节点（如第一步推理）生成 w 个可能的后续推理步骤作为该节点的子节点。这个过程重复进行，直到达到预设的最大深度或所有叶节点达到最终答案。

- **策略模型和步骤分段**：构建推理树需要清晰定义推理步骤。将数学问题解决方案转化为具有清晰步骤的形式，将**答案分成多行**，每行以行号开始，并包含该行内的推理。
- **奖励模型和剪枝**：上述提出的树生成算法计算成本高昂。当设置后续推理步骤数目为 3 和深度为 10 时，最后一次迭代需要生成 3 的 10 次方个推理步骤。因此，**使用奖励模型来剪除错误的推理步骤，提高操作效率**。具体来说，团队采用束搜索，在每次迭代中只选择少量候选项保留到下一轮。根据使用的奖励模型，剪枝实现的细节有所不同。在树生成的每次迭代中，利用来自 o1-mini 的奖励，选择最多 K 个正确的推理步骤进入下一次迭代。

### Q6：如何从推理树中推导出长思维？

一旦构建了推理树，目标就变为探索**如何从推理树转换为包含试错过程的长思维**。在该团队的框架中，推理树的每个节点都被奖励模型标注，指示该步骤是否正确或错误。具体的合成步骤如下：

- **从推理树构建捷径**：首先从推理树构建捷径，其中只包括正确答案和有效的中间步骤。从代表问题的根节点开始，找出通向正确答案叶节点的路径。如果有多个正确答案节点，则建立多条正确路径。

- **遍历推理树**：为了得到长思维，采用深度优先搜索（DFS）遍历树。这种遍历按 DFS 顺序构建路径，记录从根问题节点到正确答案叶节点的每一步，同时包括任何被标记为错误的节点的推理。

  DFS 的挑战在于它探索了庞大的搜索空间，产生了大量可能无法得到正确解决方案的试错路径。为了简化这一初始探索，团队还引入了具体的约束来缓解由于遍历路径过长导致的合成数据的复杂性。

  首先，根据节点是否位于正确路径（即捷径）上来标记树中的所有节点。遍历遵循以下规则： 

  - 正确路径上的节点：DFS 遇到正确路径上的节点时，它可能会探索导致错误结果的子节点，从而模拟试错的过程。一旦这个节点到达叶节点并被确定为错误，算法就会回**溯并切换到正确的路径继续遍历**。 
  - 不在正确路径上的节点：随机选择一个子节点进行探索，并不产生试错的分支。

  为进一步简化过程，应用了一个额外的约束：正确路径上的每个节点最多允许 K 次试错 ———— 一次在错误路径上的试错和一次在正确路径上的探索。 

  **这些约束确保 DFS 遍历专注有意义的试错探索，同时避免过度探索错误路径。**

- **从遍历路径得到长思维**：生成遍历路径并将推理附加到错误节点后，通过连接路径中的所有步骤来构建长思维，其中还包含了每个错误步骤的推理。

:cry: 初步实验表明，使用这个形式的长思维数据来训练模型的性能不佳。

为解决这个问题，团队尝试**使用 GPT-4o 来修改草稿**。GPT-4o 在保留所有推理步骤（包括错误步骤、反思和修正）的同时，增强了思维过程的连贯性和流畅性。这种方法确保最终的长思维不仅准确，而且自然流畅，模拟了包含正确和错误步骤的人类问题解决过程。

### Q7：如何评估我们的尝试方法？

除了使用特定评估指标在基准测试上测试准确率分数外，**人工审查实际案例**（输入输出）是评估数据和模型的关键步骤。因此，为了提供一种更直观的方式来评估模型在特定问题上的表现，团队构建了一个**可视化数据分析平台**。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gWicyGrm6YH575tkcscxxKbyHy221SibibVF8ycAHbNMCdY8U3fic4zibrn3bibhaBvNVzy8Pelm99gXSic2g/)

### Q8：如何训练我们的模型？

训练过程分为两个主要阶段：监督微调（SFT）和直接偏好学习（DPO）。

**第一阶段：监督微调**

- **初始阶段**：在这个初始阶段，团队专注于使用只包含正确中间步骤和最终正确答案的响应来微调模型。使用单个正确的逐步解决方案，丢弃不导向最终答案的回复。主要目的是让模型熟悉所需的响应格式。
- **旅程学习**：在第二阶段，使用构建的长思维（包含 327 个示例）进一步微调初始阶段的 SFT 模型。这个阶段旨在增强模型发现错误、自我反思、自我修正和执行回溯的能力。通过在合成的包含试错、反思的长思维数据上训练，模型对更长推理链中涉及的复杂性有更深入的理解。

**第二阶段：直接偏好学习**

在这个阶段，使用核采样（`top_p = 0.95` 和温度 `T = 0.7`）从 MATH Train 数据集为每个问题生成 20 个回复。这 20 个回复根据最终答案的正确性分类为正面和负面响应。从中，随机选择 5 个正面响应和 5 个负面响应来创建 5 对偏好对。然后，使用这些偏好对和 DPO 损失来训练模型，使其能够从正确和错误答案的比较中学习。

### Q9：什么是人类和 AI 协同标注的有效策略？

团队开发了一种人类和 AI 协作的数据标注流程，用于生成基于 MATH 数据集的高质量、长文本推理数据。通过这个流程，**将短短几行人类标注的解题方案扩展为包含数千个 token 的、符合 “旅程学习” 范式的详细推理过程**。在构建流程的过程中，我们发现了下面几种有效的标注技巧：

- **完整的思维过程**：标注者不必详细记录每一个想到的词语，但必须记录每一个尝试、反思、联想和修正的过程。这些**发散的认知路径**在日常思考中可能并未被表达成文字，甚至没有被**显式认知**。然而，捕捉这些思维转变以及背后的原因是至关重要的。这种规划和理解认知转换的能力是大语言模型从我们的数据中必须学习的核心技能。
- **补充解释常识**：人类在用语中经常省略一些可以从上下文中推断的信息，比如对前述公式的引用，或是对广为人知的理论的应用。然而，当大语言模型尝试解读人类标注时，这种省略可能导致幻觉。因此，高质量的数据必须包括对常识性知识的明确解释，以防止大模型的误解。

遵循以上两个关键要素，人类专家即可完成数据标注，这些数据精简但准确，非常利于大模型做进一步增强。下一阶段，通过设计复杂的提示词，我们通过大语言模型实现了数据扩展和增强。我们的提示词包含以下关键点：

- **数据颗粒度的增强**：提示词强调将问题解决过程分解为更细小的步骤。通过将过程拆解成细粒度且易于理解的步骤块，大语言模型能更好地掌握和内化每个概念，确保在每个阶段都有深入的理解。
- **逐步推理**：提示词控制大语言模型需频繁暂停，反思已知信息或提出下一步的操作。这种停顿模仿了学生在思考问题时的自然过程，帮助他们保持参与感和对推理过程的连接感，而不仅仅是被动地遵循指令。
- **探索者视角**：与直接呈现答案不同，大语言模型被鼓励以探索的语气进行推理，即假设自己是第一次思考这个问题。这种方式可以激发某种程度的 “好奇心”，鼓励模型批判性思考，使他们感觉自己是学习过程的一部分，而不是简单地接收信息。

## 下一步探索

团队根据的研究时间线和取得的进展，确定了几个未来探索和发展的关键方向：

- **扩展长思维的合成**：处理更复杂和多样的思维模式。
- **长思维扩展定律实验**：理解模型的性能和能力如何随着数据、模型大小和计算资源的增加而扩展。
- **细粒度、以思考为中心的评估**：更准确地衡量生成的长思维的质量和连贯性，为模型推理能力提供更深入的洞察。
- **人机协作以提高思考质量**：产生更贴近人类思维的高质量思考数据。
- **持续改进奖励和批评模型**：基于过程级奖励模型和评论模型设置，以更好地提供细粒度的监督信号。
- **推理树的合成优化**：计划探索从推理树中推导和集成长思维更复杂、有效的方法。
- **扩展训练方法**：增加预训练阶段、迭代训练、强化学习、偏好学习和 DPO。
- **持续的透明度和资源共享**：继续分享在整个科研旅程中开发的资源、观察到的结论和工具。
- **探索多代理方法**：基于在多代理系统方面的初步尝试，发现建模复杂推理和决策过程潜在的新方法。
- **完善分析工具**：进一步开发和增强分析工具。

通过追求这些途径，不仅推进我们对 o1 能力的理解和复制，还要推动 AI 研究方法的边界。
